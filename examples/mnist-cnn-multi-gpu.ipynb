{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN Multi GPU Example\n",
    "Adapted from [TensorFlow: Deep MNIST for Experts](https://www.tensorflow.org/tutorials/mnist/pros/index.html).\n",
    "Multi-GPU code is inspired by [TensorFlow: CNN - Training a Model using Multiple GPU Cards](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add path to libraries for ipython\n",
    "sys.path.append(os.path.expanduser(\"~/libs\"))\n",
    "\n",
    "import time\n",
    "import numpy as npvariable_scope\n",
    "import tensorflow as tf\n",
    "import tensortools as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_GPUS = 2\n",
    "MEMORY_DEVICE = '/cpu:0'\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "\n",
    "BATCH_SIZE = 128  # per GPU\n",
    "MAX_STEPS = 2000\n",
    "DROPOUT = 0.5\n",
    "REG = 5e-4\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(x, keep_prob, scope):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "    # Conv1\n",
    "    conv1 = tt.network.conv2d(\"Conv1\", x_image,\n",
    "                              64, 5, 5, 1, 1,\n",
    "                              weight_init=0.01,\n",
    "                              bias_init=0.1,\n",
    "                              regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                              activation=tf.nn.relu,\n",
    "                              device=MEMORY_DEVICE)\n",
    "    h_pool1 = tt.network.max_pool2d(conv1, 2, 2, 2, 2)\n",
    "\n",
    "    # Conv2\n",
    "    conv2 = tt.network.conv2d(\"Conv2\", h_pool1,\n",
    "                              128, 5, 5, 1, 1,\n",
    "                              weight_init=0.01, \n",
    "                              bias_init=0.1,\n",
    "                              regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                              device=MEMORY_DEVICE)\n",
    "    h_conv2 = tt.network.lrelu(conv2, 0.2)\n",
    "    h_pool2 = tt.network.max_pool2d(h_conv2, 2, 2, 2, 2)\n",
    "\n",
    "    # FC\n",
    "    h_pool2_flat = tf.contrib.layers.flatten(h_pool2)\n",
    "    h_fc1 = tt.network.fc(\"FC\", h_pool2_flat, 1024,\n",
    "                          weight_init=tf.contrib.layers.xavier_initializer(), \n",
    "                          bias_init=0.1,\n",
    "                          regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                          activation=tf.nn.relu,\n",
    "                          device=MEMORY_DEVICE)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Output\n",
    "    y_conv=tf.nn.softmax(tt.network.fc(\"Output\", h_fc1_drop, 10,\n",
    "                                       weight_init=tf.contrib.layers.xavier_initializer(),\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                                       bias_init=0.1,\n",
    "                                       device=MEMORY_DEVICE))\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(y, y_, scope=None):\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "        \n",
    "        # Remarks: Filter by scope might improve the the visualization in TensorBoard.\n",
    "        #          But this might require to use manual matmul()-weight decay, \n",
    "        #          because the contrib-regularizers are only evaluated once.\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        \n",
    "        total_loss = cross_entropy + tf.add_n(reg_losses)\n",
    "        \n",
    "    with tf.name_scope(\"Accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tower_loss(x, y_, keep_prob, scope):\n",
    "    \"\"\"Calculate the total loss on a single tower.\n",
    "    Args:\n",
    "        scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "    Returns:\n",
    "        Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"\n",
    "    # Build inference Graph.\n",
    "    logits = inference(x, keep_prob, scope)\n",
    "\n",
    "    # Build the portion of the Graph calculating the losses. Note that we will\n",
    "    # assemble the total_loss using a custom function below.\n",
    "    total_loss, accuracy = loss(logits, y_, scope)\n",
    "\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    #loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    #loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    #with tf.control_dependencies([loss_averages_op]):\n",
    "    #    total_loss = tf.identity(total_loss)\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g.as_default(): #, tf.device('/cpu:0'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], \"X\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], \"Y_\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"KeepProb\")\n",
    "    \n",
    "    # Create a variable to count the number of train() calls. This equals the\n",
    "    # number of batches processed * FLAGS.num_gpus.\n",
    "    global_step = tf.get_variable(\n",
    "        'global_step', [],\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "    \n",
    "    with tf.name_scope('Train'):   \n",
    "        opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        for i in xrange(NUM_GPUS):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                    # Calculate the loss for one tower of the CIFAR model. This function\n",
    "                    # constructs the entire CIFAR model but shares the variables across\n",
    "                    # all towers.\n",
    "                    t_loss, accuracy = tower_loss(x[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :],\n",
    "                                                  y_[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :], \n",
    "                                                  keep_prob, scope)\n",
    "\n",
    "                    # Reuse variables for the next tower.\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    # Calculate the gradients for the batch of data on this tower.\n",
    "                    grads = opt.compute_gradients(t_loss)\n",
    "\n",
    "                    # Keep track of the gradients across all towers.\n",
    "                    tower_grads.append(grads)\n",
    "        \n",
    "        # We must calculate the mean of each gradient.\n",
    "        # This is also the synchronization point across all towers.\n",
    "        grads = tt.training.average_gradients(tower_grads)\n",
    "        \n",
    "        # Apply the gradients to adjust the shared variables.\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        \n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(\n",
    "            MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "        # Group all updates to into a single train op.\n",
    "        train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "    \n",
    "    # start/init the session\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options,\n",
    "                                                       log_device_placement=True))\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(MAX_STEPS):\n",
    "        # each gpu gets a fraction of the batch\n",
    "        batch = mnist.train.next_batch(BATCH_SIZE * NUM_GPUS)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                                           x:batch[0],\n",
    "                                           y_: batch[1],\n",
    "                                           keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "\n",
    "        train_op.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: DROPOUT})\n",
    "        \n",
    "    duration = time.time() - start\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "                                             x: mnist.test.images[:max(1000, BATCH_SIZE * NUM_GPUS)],\n",
    "                                             y_: mnist.test.labels[:max(1000, BATCH_SIZE * NUM_GPUS)],\n",
    "                                             keep_prob: 1.0}))\n",
    "    print(\"Duration: {} sec total | {} sec/batch | {} sec/example \".format(duration,\n",
    "                                                                           duration / MAX_STEPS,\n",
    "                                                                           duration / (MAX_STEPS * BATCH_SIZE * NUM_GPUS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results\n",
    "\n",
    "Tested on DeepThought's TitanX GPUs with 0% Utility and free (0 / 12206 MiB) memory\n",
    "\n",
    "### CPU only\n",
    "\n",
    "**@ BATCH_SIZE = 128**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 712.075469971 sec total\n",
    "- 0.356037734985 sec/batch\n",
    "- 0.00278154480457 sec/example \n",
    "\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 130MiB\n",
    "- gpu1: 108MiB\n",
    "\n",
    "### 1 GPU (before using multi-GPU code):\n",
    "\n",
    "**@ BATCH_SIZE = 128**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 37.0640978813 sec total\n",
    "- 0.0185320489407 sec/batch\n",
    "- 0.000144781632349 sec/example \n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 4285MiB\n",
    "- gpu1: 108MiB\n",
    "\n",
    "### 1 GPU (store all variables on CPU)\n",
    "\n",
    "**@ BATCH_SIZE = 128**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 65.0734059811 sec total\n",
    "- 0.0325367029905 sec/batch\n",
    "- 0.000254192992114 sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 4285MiB\n",
    "- gpu1: 108MiB\n",
    "\n",
    "### 2 GPUs (Conv1@gpu:0, Conv2@gpu:1 + colocate_gradients_with_ops=True)\n",
    "\n",
    "**@ BATCH_SIZE = 128**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 40.3634259701 sec total\n",
    "- 0.020181712985 sec/batch\n",
    "- 0.000157669632696 sec/example \n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 2236 MiB\n",
    "- gpu1: 2795 MiB\n",
    "\n",
    "### 2 GPUs (Conv1@gpu:0, Conv2@gpu:1 + colocate_gradients_with_ops=False)\n",
    "\n",
    "**@ BATCH_SIZE = 128**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 46.6568570137 sec total\n",
    "- 0.0233284285069 sec/batch\n",
    "- 0.00018225334771 sec/example \n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 2236 MiB\n",
    "- gpu1: 2795 MiB\n",
    "\n",
    "### 2 GPUs (Model on GPUs, Gradient avg. on CPU)\n",
    "\n",
    "**@ BATCH_SIZE = 128 (effectively: 256)**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 86.920060873 sec total\n",
    "- 0.0434600304365 sec/batch\n",
    "- 0.000169765743893 sec/example \n",
    "\n",
    "\n",
    "- 75.5817620754 sec total\n",
    "- 0.0377908810377 sec/batch\n",
    "- 0.000147620629054 sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 1214 MiB\n",
    "- gpu1: 1212 MiB\n",
    "\n",
    "**@ BATCH_SIZE = 256 (effectively: 512)**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 109.699135065 sec total\n",
    "- 0.0548495675325 sec/batch\n",
    "- 0.000107128061587 sec/example \n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 2238 MiB\n",
    "- gpu1: 2236 MiB\n",
    "\n",
    "**@ BATCH_SIZE = 512 (effectively: 1024)**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 168.136204004 sec total\n",
    "- 0.0840681020021 sec/batch\n",
    "- 0.0000820977558615 sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 4324 MiB\n",
    "- gpu1: 4348 MiB\n",
    "\n",
    "### 3 GPUs (Model on GPUs, Gradient avg. on CPU)\n",
    "\n",
    "**@ BATCH_SIZE = 256 (effectively: 768)**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 117.819902897 sec total\n",
    "- 0.0589099514484 sec/batch\n",
    "- 0.0000767056659485 sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 2236 MiB\n",
    "- gpu1: 2236 MiB\n",
    "- gpu2: 2274 MiB\n",
    "\n",
    "**@ BATCH_SIZE = 256 (effectively: 768)**\n",
    "*without storing variables on CPU!*\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 91.0233440399 sec total\n",
    "- 0.04551167202 sec/batch\n",
    "- 0.0000592599896093 sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 2238 MiB\n",
    "- gpu1: 2236 MiB\n",
    "- gpu2: 2236 MiB\n",
    "\n",
    "**@ BATCH_SIZE = 768 (effectively: 2304)**\n",
    "\n",
    "#### Duration\n",
    "\n",
    "- 230.154171944 sec total\n",
    "- 0.115077085972 sec/batch\n",
    "- 0.0000499466518975e sec/example\n",
    "\n",
    "#### Memory\n",
    "\n",
    "- gpu0: 4348 MiB\n",
    "- gpu1: 4348 MiB\n",
    "- gpu2: 4352 MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
