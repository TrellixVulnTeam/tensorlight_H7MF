{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorTools Runtime Demo\n",
    "\n",
    "Demonstrates the usage of the runtime using a simple autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Force matplotlib to use inline rendering\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add path to libraries for ipython\n",
    "sys.path.append(os.path.expanduser(\"~/libs\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensortools as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "REG_LAMBDA = 5e-4\n",
    "NUM_GPUS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File mnist.h5 has already been downloaded.\n",
      "File mnist.h5 has already been downloaded.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tt.datasets.moving_mnist.MovingMNISTTrainDataset(BATCH_SIZE * NUM_GPUS,\n",
    "                                                                 1)\n",
    "dataset_valid = tt.datasets.moving_mnist.MovingMNISTValidDataset(BATCH_SIZE * NUM_GPUS,\n",
    "                                                                 1)\n",
    "#dataset_test = tt.datasets.moving_mnist.MovingMNISTTestDataset(BATCH_SIZE * NUM_GPUS,\n",
    "#                                                               1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleAutoencoderModel(tt.model.AbstractModel):\n",
    "    \"\"\"Simple neuronal autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, targets, reg_lambda,\n",
    "                 scope=None, is_training=False):\n",
    "        self._scope = scope  # TODO: move to base class (call is device scope?)\n",
    "        self._is_training = is_training  # TODO: move to base class\n",
    "        super(SimpleAutoencoderModel, self).__init__(inputs, targets, reg_lambda)\n",
    "    \n",
    "    @tt.utils.attr.lazy_property\n",
    "    def predictions(self):\n",
    "        x = tf.contrib.layers.flatten(self._inputs)\n",
    "        encoded = tt.network.fc(\"FC_Enc\", x, 64,\n",
    "                                weight_init=tf.contrib.layers.xavier_initializer(),\n",
    "                                bias_init=0.0)\n",
    "        representation = encoded\n",
    "        decoded = tt.network.fc(\"FC_Dec\", representation, self.input_shape[1] * self.input_shape[2] * self.input_shape[3],\n",
    "                                weight_init=tf.contrib.layers.xavier_initializer(),\n",
    "                                bias_init=0.0)\n",
    "        \n",
    "        return tf.reshape(decoded, [-1, 1, self.input_shape[1], self.input_shape[2], self.input_shape[3]])\n",
    "\n",
    "    @tt.utils.attr.lazy_property\n",
    "    def loss(self):\n",
    "        return tt.loss.bce(self.predictions, self._targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runtime = tt.core.MultiGpuRuntime()\n",
    "runtime.register_datasets(dataset_train, dataset_valid)\n",
    "runtime.register_model(\n",
    "    lambda x, y, scope, is_training : SimpleAutoencoderModel(x, y,\n",
    "                                                             reg_lambda=REG_LAMBDA,\n",
    "                                                             scope=scope,\n",
    "                                                             is_training=is_training))\n",
    "runtime.setup_feeding(tt.core.FEEDING_AUTOENCODER)\n",
    "runtime.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@    10: loss:     0.324, total-loss:     0.324 ( 8208.5 examples/sec,  0.01 sec/batch)\n",
      "@    20: loss:     0.219, total-loss:     0.219 ( 7293.1 examples/sec,  0.01 sec/batch)\n",
      "@    30: loss:     0.171, total-loss:     0.171 ( 8455.9 examples/sec,  0.01 sec/batch)\n",
      "@    40: loss:     0.158, total-loss:     0.158 (12196.8 examples/sec,  0.01 sec/batch)\n",
      "@    50: loss:     0.159, total-loss:     0.159 ( 9608.7 examples/sec,  0.01 sec/batch)\n",
      "@    60: loss:     0.146, total-loss:     0.146 ( 6093.3 examples/sec,  0.02 sec/batch)\n",
      "@    70: loss:     0.165, total-loss:     0.165 ( 9495.4 examples/sec,  0.01 sec/batch)\n",
      "@    80: loss:     0.138, total-loss:     0.138 ( 7722.7 examples/sec,  0.01 sec/batch)\n",
      "@    90: loss:     0.158, total-loss:     0.158 (11041.0 examples/sec,  0.01 sec/batch)\n",
      "@   100: loss:     0.130, total-loss:     0.130 (10098.9 examples/sec,  0.01 sec/batch)\n",
      "\n",
      "@   100: Starting validation (batch-size: 48, dataset-size: 10000):\n",
      "9984/9984 [================================] - 1s - loss: 0.1410     \n",
      "\n",
      "@   110: loss:     0.136, total-loss:     0.136 ( 7944.4 examples/sec,  0.01 sec/batch)\n",
      "@   120: loss:     0.125, total-loss:     0.125 ( 9516.1 examples/sec,  0.01 sec/batch)\n",
      "@   130: loss:     0.123, total-loss:     0.123 ( 8890.6 examples/sec,  0.01 sec/batch)\n",
      "@   140: loss:     0.126, total-loss:     0.126 (10103.2 examples/sec,  0.01 sec/batch)\n",
      "@   150: loss:     0.127, total-loss:     0.127 ( 6557.4 examples/sec,  0.01 sec/batch)\n",
      "@   160: loss:     0.129, total-loss:     0.129 ( 9129.8 examples/sec,  0.01 sec/batch)\n",
      "@   170: loss:     0.119, total-loss:     0.119 ( 7914.2 examples/sec,  0.01 sec/batch)\n",
      "@   180: loss:     0.124, total-loss:     0.124 (10637.3 examples/sec,  0.01 sec/batch)\n",
      "@   190: loss:     0.125, total-loss:     0.125 ( 7936.4 examples/sec,  0.01 sec/batch)\n",
      "@   200: loss:     0.115, total-loss:     0.115 ( 6078.6 examples/sec,  0.02 sec/batch)\n",
      "@   210: loss:     0.117, total-loss:     0.117 ( 7678.8 examples/sec,  0.01 sec/batch)\n",
      "@   220: loss:     0.123, total-loss:     0.123 ( 6300.0 examples/sec,  0.02 sec/batch)\n",
      "@   230: loss:     0.135, total-loss:     0.135 ( 8414.4 examples/sec,  0.01 sec/batch)\n",
      "@   240: loss:     0.140, total-loss:     0.140 ( 7377.3 examples/sec,  0.01 sec/batch)\n",
      "@   250: loss:     0.117, total-loss:     0.117 (11581.8 examples/sec,  0.01 sec/batch)\n",
      "@   260: loss:     0.114, total-loss:     0.114 (10122.3 examples/sec,  0.01 sec/batch)\n",
      "@   270: loss:     0.122, total-loss:     0.122 ( 8535.7 examples/sec,  0.01 sec/batch)\n",
      "@   280: loss:     0.126, total-loss:     0.126 ( 7035.9 examples/sec,  0.01 sec/batch)\n",
      "@   290: loss:     0.123, total-loss:     0.123 (12063.4 examples/sec,  0.01 sec/batch)\n",
      "@   300: loss:     0.132, total-loss:     0.132 ( 6330.0 examples/sec,  0.02 sec/batch)\n",
      "@   310: loss:     0.124, total-loss:     0.124 (11153.8 examples/sec,  0.01 sec/batch)\n",
      "@   320: loss:     0.127, total-loss:     0.127 (10796.1 examples/sec,  0.01 sec/batch)\n",
      "@   330: loss:     0.121, total-loss:     0.121 ( 8042.3 examples/sec,  0.01 sec/batch)\n",
      "@   340: loss:     0.122, total-loss:     0.122 ( 6645.0 examples/sec,  0.01 sec/batch)\n",
      "@   350: loss:     0.114, total-loss:     0.114 ( 7802.3 examples/sec,  0.01 sec/batch)\n",
      "@   360: loss:     0.115, total-loss:     0.115 ( 7666.4 examples/sec,  0.01 sec/batch)\n",
      "@   370: loss:     0.113, total-loss:     0.113 ( 8915.4 examples/sec,  0.01 sec/batch)\n",
      "@   380: loss:     0.110, total-loss:     0.110 ( 7426.3 examples/sec,  0.01 sec/batch)\n",
      "@   390: loss:     0.112, total-loss:     0.112 ( 8303.8 examples/sec,  0.01 sec/batch)\n",
      "@   400: loss:     0.112, total-loss:     0.112 ( 8002.0 examples/sec,  0.01 sec/batch)\n",
      "@   410: loss:     0.120, total-loss:     0.120 ( 9206.0 examples/sec,  0.01 sec/batch)\n",
      "@   420: loss:     0.117, total-loss:     0.117 (10448.5 examples/sec,  0.01 sec/batch)\n",
      "@   430: loss:     0.119, total-loss:     0.119 (12337.7 examples/sec,  0.01 sec/batch)\n",
      "@   440: loss:     0.132, total-loss:     0.132 (10065.1 examples/sec,  0.01 sec/batch)\n",
      "@   450: loss:     0.125, total-loss:     0.125 ( 9536.1 examples/sec,  0.01 sec/batch)\n",
      "@   460: loss:     0.126, total-loss:     0.126 ( 8893.1 examples/sec,  0.01 sec/batch)\n",
      "@   470: loss:     0.129, total-loss:     0.129 ( 6712.8 examples/sec,  0.01 sec/batch)\n",
      "@   480: loss:     0.123, total-loss:     0.123 ( 7838.8 examples/sec,  0.01 sec/batch)\n",
      "@   490: loss:     0.117, total-loss:     0.117 ( 9815.1 examples/sec,  0.01 sec/batch)\n",
      "@   500: loss:     0.115, total-loss:     0.115 (10897.8 examples/sec,  0.01 sec/batch)\n",
      "\n",
      "@   500: Starting validation (batch-size: 48, dataset-size: 10000):\n",
      "9984/9984 [================================] - 1s - loss: 0.1189     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "runtime.train(max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@   500: Starting validation (batch-size: 48, dataset-size: 10000):\n",
      "9984/9984 [================================] - 1s - loss: 0.1188     \n"
     ]
    }
   ],
   "source": [
    "runtime.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset registered. Skipping.\n"
     ]
    }
   ],
   "source": [
    "runtime.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
