{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Utils Example\n",
    "\n",
    "This example demonstrates the usage of the tensorlight.utils.image module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force matplotlib to use inline rendering\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add path to libraries for ipython\n",
    "sys.path.append(os.path.expanduser(\"~/libs\"))\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorlight as light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = light.utils.image.read(\"assets/color250x250.jpg\")\n",
    "img_gray = light.utils.image.read(\"assets/color250x250.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(img_color.shape, img_color.dtype)\n",
    "print(img_gray.shape, img_gray.dtype)\n",
    "\n",
    "light.visualization.display_array(img_color)\n",
    "light.visualization.display_array(img_gray)\n",
    "\n",
    "light.visualization.display_batch(np.expand_dims(img_color, 0))\n",
    "light.visualization.display_batch(np.expand_dims(img_gray, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = light.utils.image.read(\"assets/example250x250.jpg\")\n",
    "img_gray = light.utils.image.read(\"assets/example250x250.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "img_color64 = img_color / 255.0\n",
    "img_color32 = np.float32(img_color / 255.0)\n",
    "\n",
    "print(img_color32.shape, img_color32.dtype)\n",
    "print(img_color64.shape, img_color64.dtype)\n",
    "print(img_gray.shape, img_gray.dtype)\n",
    "\n",
    "light.visualization.display_array(img_color32)\n",
    "light.visualization.display_array(img_color64)\n",
    "light.visualization.display_array(img_gray)\n",
    "\n",
    "light.visualization.display_batch(np.expand_dims(img_color32, 0))\n",
    "light.visualization.display_batch(np.expand_dims(img_color64, 0))\n",
    "light.visualization.display_batch(np.expand_dims(img_gray, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_gray_img = light.utils.image.to_grayscale(img_color)\n",
    "print(color_to_gray_img.shape, color_to_gray_img.dtype)\n",
    "\n",
    "light.visualization.display_array(color_to_gray_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_to_color_img = light.utils.image.to_rgb(img_gray)\n",
    "print(gray_to_color_img.shape, gray_to_color_img.dtype)\n",
    "\n",
    "light.visualization.display_array(gray_to_color_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color_resized = light.utils.image.resize(img_color, 0.5)\n",
    "print(img_color_resized.shape, img_color_resized.dtype)\n",
    "img_gray_resized = light.utils.image.resize(img_gray, size=(50, 100))\n",
    "print(img_gray_resized.shape, img_gray_resized.dtype)\n",
    "img_gray_resized_odd = light.utils.image.resize(img_gray_resized, 0.333333)\n",
    "print(img_gray_resized_odd.shape, img_gray_resized_odd.dtype)\n",
    "\n",
    "light.visualization.display_array(img_color_resized)\n",
    "light.visualization.display_array(img_gray_resized)\n",
    "light.visualization.display_array(img_gray_resized_odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_color_resized.shape, img_color_resized.dtype)\n",
    "img_color_resized_float32 = np.float32(img_color_resized / 255.0)\n",
    "print(img_color_resized.shape, img_color_resized.dtype)\n",
    "print(img_color_resized_float32.shape, img_color_resized_float32.dtype)\n",
    "print(img_gray.shape, img_gray.dtype)\n",
    "\n",
    "light.utils.image.write(\"out/img_color_resized_uint8.png\",\n",
    "                     img_color_resized)\n",
    "light.utils.image.write(\"out/img_color_resized_float32.png\",\n",
    "                     img_color_resized_float32)\n",
    "light.utils.image.write_as_binary(\"out/img_gray.bin\",\n",
    "                               img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pad or crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = light.utils.image.read(\"assets/color250x250.jpg\")\n",
    "print(\"Original\", img.shape)\n",
    "img_same = light.utils.image.pad_or_crop(img, img.shape)\n",
    "print(\"Same\", img_same.shape)\n",
    "\n",
    "img_odd_pad = light.utils.image.pad_or_crop(img, [251, 251])\n",
    "print(\"Odd pad\", img_odd_pad.shape)\n",
    "print(\"Original unchanged\", img.shape)\n",
    "img_even_pad = light.utils.image.pad_or_crop(img, [300, 300], pad_value=0)\n",
    "print(\"Even pad\", img_even_pad.shape)\n",
    "print(\"Original unchanged\", img.shape)\n",
    "\n",
    "img_odd_crop = light.utils.image.pad_or_crop(img, [249, 249])\n",
    "print(\"Odd crop\", img_odd_crop.shape)\n",
    "print(\"Original unchanged\", img.shape)\n",
    "img_even_crop = light.utils.image.pad_or_crop(img, [200, 200], pad_value=0)\n",
    "print(\"Even crop\", img_even_crop.shape)\n",
    "print(\"Original unchanged\", img.shape)\n",
    "\n",
    "img_mixed = light.utils.image.pad_or_crop(img, [300, 200], pad_value=0)\n",
    "print(\"Mixed\", img_mixed.shape)\n",
    "print(\"Original unchanged\", img.shape)\n",
    "\n",
    "light.visualization.display_array(img)\n",
    "light.visualization.display_array(img_same)\n",
    "light.visualization.display_array(img_even_pad)\n",
    "light.visualization.display_array(img_even_crop)\n",
    "light.visualization.display_array(img_mixed)\n",
    "\n",
    "# test that modifying the copy does not change the original\n",
    "img_even_pad.fill(99)\n",
    "img_odd_pad.fill(99)\n",
    "img_even_crop.fill(99)\n",
    "img_odd_crop.fill(99)\n",
    "img_reloaded = light.utils.image.read(\"assets/color250x250.jpg\")\n",
    "print(\"Check that image manipulation did not change the original:\",\n",
    "      np.all(img == img_reloaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
