{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN Board Example\n",
    "Adapted from [TensorFlow: Deep MNIST for Experts](https://www.tensorflow.org/tutorials/mnist/pros/index.html).\n",
    "\n",
    "Demonstrates the usage of the *tensortools/board* module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add path to libraries for ipython\n",
    "sys.path.append(os.path.expanduser(\"~/libs\"))\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as npvariable_scope\n",
    "import tensorflow as tf\n",
    "import tensortools as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "MAX_STEPS = 2000\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "REG = 5e-4\n",
    "TRAIN_DIR = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def inference(x, dropout_keep_prob, filter_step_index):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "    # Conv1\n",
    "    conv1 = tt.network.conv2d(\"Conv1\", x_image,\n",
    "                              32, 5, 5, 1, 1,\n",
    "                              weight_init=0.01, \n",
    "                              bias=0.1,\n",
    "                              regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                              activation=tf.nn.relu)\n",
    "    tt.board.activation_summary(conv1, True, scope=\"Conv1\")\n",
    "    tt.board.conv_image_summary(\"conv1_out\", conv1)\n",
    "    \n",
    "    with tf.variable_scope(\"Conv1\", reuse=True):\n",
    "        kernel = tf.get_variable(\"W\")\n",
    "    tt.board.conv_filter_image_summary(\"conv1_filters\", kernel);\n",
    "    # demo for how to show an image instance for multiple step,\n",
    "    # without overriding the previous image\n",
    "    tt.board.conv_filter_image_summary(\"conv1_filters_\" + filter_step_index, kernel);\n",
    "    \n",
    "    h_pool1 = tt.network.max_pool2d(conv1, 2, 2, 2, 2)\n",
    "\n",
    "    # Conv2\n",
    "    conv2 = tt.network.conv2d(\"Conv2\", h_pool1,\n",
    "                              64, 5, 5, 1, 1,\n",
    "                              weight_init=0.01, \n",
    "                              bias=0.1,\n",
    "                              regularizer=tf.contrib.layers.l2_regularizer(REG))\n",
    "    h_conv2 = tt.network.lrelu(conv2, 0.2)\n",
    "    tt.board.activation_summary(h_conv2, scope=\"Conv2\")\n",
    "    h_pool2 = tt.network.max_pool2d(h_conv2, 2, 2, 2, 2)\n",
    "    \n",
    "    # Conv3\n",
    "    conv3 = tt.network.conv2d(\"Conv3\", h_pool2,\n",
    "                              64, 5, 5, 1, 1,\n",
    "                              weight_init=0.01, \n",
    "                              bias=0.1,\n",
    "                              regularizer=tf.contrib.layers.l2_regularizer(REG))\n",
    "    h_conv3 = tf.nn.relu(conv3, name=\"ReLu\")\n",
    "    tt.board.activation_summary(h_conv3, scope=\"Conv3\")\n",
    "\n",
    "    # FC\n",
    "    h_pool2_flat = tf.reshape(h_conv3, [-1, 7*7*64])\n",
    "    fc1 = tt.network.fc(\"FC\", h_pool2_flat, 1024,\n",
    "                          weight_init=tf.contrib.layers.xavier_initializer(), \n",
    "                          bias=0.1,\n",
    "                          regularizer=tf.contrib.layers.l2_regularizer(REG))\n",
    "    h_fc1 = tf.nn.relu(fc1, name=\"ReLu\")\n",
    "    tt.board.activation_summary(h_fc1, scope=\"FC\")\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, dropout_keep_prob)\n",
    "\n",
    "    # Output\n",
    "    y_conv=tf.nn.softmax(tt.network.fc(\"Output\", h_fc1_drop, 10,\n",
    "                                       weight_init=tf.contrib.layers.xavier_initializer(),\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(REG),\n",
    "                                       bias=0.1))\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(output, label, global_step):\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(label * tf.log(output), reduction_indices=[1]), name=\"cross_entropy\")\n",
    "        \n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        print(\"{} regularization losses in total.\".format(len(reg_losses)))\n",
    "        total_loss = tf.add(cross_entropy, tf.add_n(reg_losses), name=\"total_loss\")\n",
    "        \n",
    "        # Generate moving averages of all losses and associated summaries\n",
    "        cost_averages_op = tt.board.loss_summary([total_loss, cross_entropy] + reg_losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        with tf.control_dependencies([cost_averages_op]):\n",
    "            opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "            grads = opt.compute_gradients(total_loss)\n",
    "        \n",
    "        # Apply gradients\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        \n",
    "        tt.board.variables_histogram_summary()\n",
    "        \n",
    "        # Add histograms for gradients\n",
    "        tt.board.gradients_histogram_summary(grads)\n",
    "        \n",
    "        with tf.control_dependencies([apply_gradient_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    with tf.name_scope(\"Accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(label,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, 784], \"X\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], \"Y_\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"KeepProb\")\n",
    "    \n",
    "    filter_step_index = tf.placeholder(tf.string, name=\"step_index\")\n",
    "    output = inference(x, keep_prob, filter_step_index)\n",
    "    train_op = train(output, y_, global_step)\n",
    "    accuracy_op = accuracy(output, y_)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    gpu_options = tf.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=0.5,\n",
    "        allow_growth=True)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Visualize graph\n",
    "        tt.visualization.show_graph(sess.graph_def)\n",
    "        \n",
    "        # Create a TensorBoard Writer of all summaries\n",
    "        summary_writer = tf.train.SummaryWriter(TRAIN_DIR, sess.graph)\n",
    "\n",
    "        for step in range(MAX_STEPS + 1):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            sess.run(train_op, feed_dict={x: batch[0],\n",
    "                                          y_: batch[1],\n",
    "                                          keep_prob: DROPOUT,\n",
    "                                          filter_step_index: \"{:05d}\".format(step)})\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                train_accuracy = sess.run(accuracy_op,\n",
    "                                          feed_dict={x:batch[0],\n",
    "                                                     y_: batch[1],\n",
    "                                                     keep_prob: 1.0,\n",
    "                                                     filter_step_index: \"{:05d}\".format(step)})\n",
    "                \n",
    "                num_examples_per_step = BATCH_SIZE\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = ('%s: step %d, acc = %.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "                print (format_str % (datetime.now().time(), step, train_accuracy,\n",
    "                                     examples_per_sec, sec_per_batch))\n",
    "                \n",
    "            if step % 100 == 0:\n",
    "                summary_str = sess.run(summary_op,\n",
    "                                      feed_dict={x:batch[0],\n",
    "                                                 y_: batch[1],\n",
    "                                                 keep_prob: 1.0,\n",
    "                                                 filter_step_index: \"{:05d}\".format(step)})\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                # flush data because there can be problems writing the data\n",
    "                summary_writer.flush() \n",
    "        \n",
    "        test_acc = sess.run(accuracy_op, \n",
    "                            feed_dict={x: mnist.test.images[:1000],\n",
    "                                       y_: mnist.test.labels[:1000],\n",
    "                                       keep_prob: 1.0,\n",
    "                                       filter_step_index: \"TEST\"})\n",
    "        print(\"Test accuracy: %g\" % test_acc)\n",
    "        \n",
    "print(\"DONE! All outputs have been written to: {0}\".format(TRAIN_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
